{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import mysql.connector\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import nvda_config1\n",
    "\n",
    "import twelvedata\n",
    "from twelvedata import TDClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Including this login info just in case \n",
    "Cpassword = nvda_config1.mysql_password\n",
    "td = TDClient(apikey=nvda_config1.twelve_api_nvda) \n",
    "base_url = 'https://api.twelvedata.com'\n",
    "\n",
    "# Establish MySQL object\n",
    "mydb = mysql.connector.connect(\n",
    "    host=\"localhost\",   \n",
    "    user=\"root\",         \n",
    "    password=Cpassword,  \n",
    "    database=\"nvdav1\"   \n",
    ")\n",
    "\n",
    "mycursor = mydb.cursor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Data:\n",
      "   id symbol             datetime    open    high     low   close   volume\n",
      "0   1   NVDA  2024-07-19 15:59:00  118.01  118.20  117.95  117.99  3212446\n",
      "1   2   NVDA  2024-07-19 15:58:00  117.93  118.07  117.92  118.01  1192224\n",
      "2   3   NVDA  2024-07-19 15:57:00  117.86  118.00  117.83  117.94   905114\n",
      "3   4   NVDA  2024-07-19 15:56:00  117.87  117.90  117.80  117.86   770476\n",
      "4   5   NVDA  2024-07-19 15:55:00  117.88  118.08  117.84  117.86  1034337\n",
      "\n",
      "TI Data:\n",
      "   id symbol           bydatetime     macd  bysignal  byhistogram     ema  \\\n",
      "0   1   NVDA  2024-07-19 15:59:00 -0.09599  -0.11612      0.02013  117.95   \n",
      "1   2   NVDA  2024-07-19 15:58:00 -0.10684  -0.12115      0.01432  117.94   \n",
      "2   3   NVDA  2024-07-19 15:57:00 -0.12065  -0.12473      0.00409  117.92   \n",
      "3   4   NVDA  2024-07-19 15:56:00 -0.12939  -0.12575     -0.00364  117.92   \n",
      "4   5   NVDA  2024-07-19 15:55:00 -0.13053  -0.12484     -0.00569  117.93   \n",
      "\n",
      "     rsi  \n",
      "0  46.19  \n",
      "1  46.89  \n",
      "2  43.18  \n",
      "3  38.64  \n",
      "4  38.82  \n"
     ]
    }
   ],
   "source": [
    "# Read CSV files and create pandas dataframes\n",
    "# Stock prices foor NVDA historical for 1 yeaar at 1 minute intervals\n",
    "df_stock = pd.read_csv('C:/Users/micha/OneDrive/Desktop/NVDALinearLogisticAnalysis/stock_ticks.csv')\n",
    "# technical inidcators foor NVDA historical for 1 yeaar at 1 minute intervals\n",
    "df_ti = pd.read_csv('C:/Users/micha/OneDrive/Desktop/NVDALinearLogisticAnalysis/technical_indicators.csv')\n",
    "# company info\n",
    "df_nvda_info = pd.read_csv('C:/Users/micha/OneDrive/Desktop/NVDALinearLogisticAnalysis/nvda_profile.csv')\n",
    "# new data for recent days downlaoded by the minute. Will have errors\n",
    "df_recent_new = pd.read_csv('C:/Users/micha/OneDrive/Desktop/NVDALinearLogisticAnalysis/nvda_new.csv')\n",
    "# data for nvda earnings\n",
    "df_nvda_earnings = pd.read_csv('C:/Users/micha/OneDrive/Desktop/NVDALinearLogisticAnalysis/NVDA_earnings.csv')\n",
    "# aux historical\n",
    "df_aux = pd.read_csv('C:/Users/micha/OneDrive/Desktop/NVDALinearLogisticAnalysis/aux_historical_data.csv')\n",
    "# spx historical\n",
    "df_spx = pd.read_csv('C:/Users/micha/OneDrive/Desktop/NVDALinearLogisticAnalysis/spx_historical_data.csv')\n",
    "# Display the first few rows of each dataframe to verify the data\n",
    "print(\"Stock Data:\")\n",
    "print(df_stock.head())\n",
    "print(\"\\nTI Data:\")\n",
    "print(df_ti.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files being read in above are results of the extract_from_sql_tool which taps into the MYSQL server and extracts the entirety of the table. These tables are not altered and so will contain all information gathered on that entity. Keeping the code above to run everytime is ok. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****TO DO***********\n",
    "GENERATE CODE TO LIST NAMES OF ATTRIBUTES\n",
    "UNIFY NAMES AND DATA TYPES\n",
    "GENERATE CODE TO PULL MOST COMMON MINIMUM AND MAXIMUM DATES\n",
    "TRIM AND MERGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in AUX historical:\n",
      "Index(['datetime', 'open', 'high', 'low', 'close', 'macd', 'macd_signal',\n",
      "       'macd_hist', 'ema', 'rsi'],\n",
      "      dtype='object')\n",
      "\n",
      "Columns in SPX historical:\n",
      "Index(['datetime', 'open', 'high', 'low', 'close', 'volume', 'macd',\n",
      "       'macd_signal', 'macd_hist', 'ema', 'rsi'],\n",
      "      dtype='object')\n",
      "Columns in Stock Data:\n",
      "Index(['id', 'symbol', 'datetime', 'open', 'high', 'low', 'close', 'volume'], dtype='object')\n",
      "\n",
      "Columns in Technical Indicators Data:\n",
      "Index(['id', 'symbol', 'bydatetime', 'macd', 'bysignal', 'byhistogram', 'ema',\n",
      "       'rsi'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(\"\\nColumns in NVDA Earnings Data:\")\n",
    "#print(df_nvda_earnings.columns)\n",
    "#print(\"\\nColumns in NVDA Info Data:\")\n",
    "#print(df_nvda_info.columns)\n",
    "#print(\"\\nColumns in NVDA Recent New Data:\")\n",
    "#print(df_recent_new.columns)\n",
    "\n",
    "print(\"\\nColumns in AUX historical:\")\n",
    "print(df_aux.columns)\n",
    "print(\"\\nColumns in SPX historical:\")\n",
    "print(df_spx.columns)\n",
    "print(\"Columns in Stock Data:\")\n",
    "print(df_stock.columns)\n",
    "print(\"\\nColumns in Technical Indicators Data:\")\n",
    "print(df_ti.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns for AUX\n",
    "df_aux.rename(columns={\n",
    "    \"open\": \"aux_open\", \n",
    "    \"close\": \"aux_close\", \n",
    "    \"aux_high\": 'high',\n",
    "    \"aux_low\": 'low',\n",
    "    \"aux_close\": 'close', \n",
    "    \"aux_macd\":'macd',\n",
    "    \"aux_macd_signal\": 'macd_signal',\n",
    "    \"aux_macd_hist\": 'macd_hist',\n",
    "    \"aux_ema\": 'ema',\n",
    "    \"aux_rsi\": 'rsi'\n",
    "}, inplace=True)\n",
    "\n",
    "# Renaming columns for SPX\n",
    "df_spx.rename(columns={  \n",
    "    \"open\": \"spx_open\", \n",
    "    \"close\": \"spx_close\", \n",
    "    \"spx_high\": 'high',\n",
    "    \"spx_low\": 'low',\n",
    "    \"spx_close\": 'close', \n",
    "    \"spx_macd\":'macd',\n",
    "    \"spx_macd_signal\": 'macd_signal',\n",
    "    \"spx_macd_hist\": 'macd_hist',\n",
    "    \"spx_ema\": 'ema',\n",
    "    \"spx_rsi\": 'rsi'\n",
    "}, inplace=True)\n",
    "\n",
    "df_stock.drop(columns=['id', 'symbol'], inplace=True)\n",
    "df_ti.drop(columns=['id', 'symbol'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4416\\4016403756.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_stock_ti\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_stock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_ti\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'datetime'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1298\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'datetime'"
     ]
    }
   ],
   "source": [
    "df_stock_ti = pd.merge(df_stock, df_ti, on='datetime', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are duplicate datetime entries.\n",
      "                  datetime    open    high     low   close    volume\n",
      "0      2024-07-19 15:59:00  118.01  118.20  117.95  117.99   3212446\n",
      "1      2024-07-19 15:58:00  117.93  118.07  117.92  118.01   1192224\n",
      "2      2024-07-19 15:57:00  117.86  118.00  117.83  117.94    905114\n",
      "3      2024-07-19 15:56:00  117.87  117.90  117.80  117.86    770476\n",
      "4      2024-07-19 15:55:00  117.88  118.08  117.84  117.86   1034337\n",
      "...                    ...     ...     ...     ...     ...       ...\n",
      "75060  2024-07-25 09:34:00  111.60  112.25  111.50  111.64   2567340\n",
      "75061  2024-07-25 09:33:00  112.43  112.50  111.51  111.59   3665020\n",
      "75062  2024-07-25 09:32:00  113.29  113.30  112.30  112.41   2248988\n",
      "75063  2024-07-25 09:31:00  113.34  113.91  113.05  113.29   1471099\n",
      "75064  2024-07-25 09:30:00  113.08  113.74  112.91  113.34  21406663\n",
      "\n",
      "[10008 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# While experimenting downloading via api there are diplicates needing to be removed\n",
    "\n",
    "duplicate_entries = df_stock['datetime'].duplicated().any()\n",
    "if duplicate_entries:\n",
    "    print(\"There are duplicate datetime entries.\")\n",
    "else:\n",
    "    print(\"No duplicate datetime entries found.\")\n",
    "\n",
    "duplicate_entries = df_stock[df_stock['datetime'].duplicated(keep=False)]\n",
    "print(duplicate_entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in: df\n",
      "Index(['date', 'time', 'eps_estimate', 'eps_actual', 'difference',\n",
      "       'surprise_prc'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "Columns in: df\n",
      "Index(['id', 'symbol', 'country', 'currency', 'exchange', 'ipo',\n",
      "       'marketCapitalization', 'name', 'phone', 'shareOutstanding', 'ticker',\n",
      "       'weburl', 'logo', 'finnhubIndustry'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "Columns in: df\n",
      "Index(['id', 'symbol', 'datetime', 'price'], dtype='object')\n",
      "\n",
      "\n",
      "Columns in: df\n",
      "Index(['id', 'symbol', 'datetime', 'open', 'high', 'low', 'close', 'volume'], dtype='object')\n",
      "\n",
      "\n",
      "Columns in: df\n",
      "Index(['id', 'symbol', 'bydatetime', 'macd', 'bysignal', 'byhistogram', 'ema',\n",
      "       'rsi'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "Columns in: df\n",
      "Index(['datetime', 'open', 'high', 'low', 'close', 'macd', 'macd_signal',\n",
      "       'macd_hist', 'ema', 'rsi'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "Columns in: df\n",
      "Index(['datetime', 'open', 'high', 'low', 'close', 'volume', 'macd',\n",
      "       'macd_signal', 'macd_hist', 'ema', 'rsi'],\n",
      "      dtype='object')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_df_columns(df_list):\n",
    "    for df in df_list:\n",
    "        print(f\"Columns in: df\")\n",
    "        print(df.columns)\n",
    "        print(\"\\n\")\n",
    "dfs = [df_nvda_earnings, df_nvda_info, df_recent_new, df_stock, df_ti, df_aux, df_spx]\n",
    "print_df_columns(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
